{
 "metadata": {
  "name": "",
  "signature": "sha256:3f6874d86b9f43405bd524ab0742c3893ebf19a728bc6bdbdec4f65b7be9631a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TAL - TME3 - Classification de documents"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Chargement du corpus movies1000 (lecture de fichiers)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import nltk.corpus.reader as pt\n",
      "\n",
      "def getDocs(path):\n",
      "    rdr = pt.CategorizedPlaintextCorpusReader(path, r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n",
      "    docs = [[rdr.raw(fileids=[f]) for f in rdr.fileids(c) ] for c in rdr.categories()]\n",
      "    return docs\n",
      "\n",
      "def getAllDocs(docs):\n",
      "    all_docs = docs[0] + docs[1]\n",
      "    return all_docs\n",
      "\n",
      "def getAllLabels(docs):\n",
      "    len_sad = len(docs[0])\n",
      "    len_happy = len(docs[1])\n",
      "    all_labels = np.ones(len_sad + len_happy)\n",
      "    all_labels[:len_happy] = 0\n",
      "    return all_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cr\u00e9ation des stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import download\n",
      "\n",
      "def nltkDownload():\n",
      "    \"\"\" Vous n'aurez surement pas les stopwords, il vous faudra les t\u00e9l\u00e9charger.\n",
      "    \"\"\"\n",
      "    download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "\n",
      "def makeNltkStopWords(languages=['french', 'english', 'german', 'spanish']):\n",
      "    stop_words = []\n",
      "    for l in languages:\n",
      "        for w in stopwords.words(l):\n",
      "           stop_words.append(w.encode('utf-8')) #w.decode('utf-8') buggait... avec certains caract\u00e8res\n",
      "    return stop_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Vectorisation et normalisation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.feature_extraction.text as txt\n",
      "    \n",
      "def fromAllDocsToBow(all_docs, strip_accents=u'ascii', lowercase=True, \\\n",
      "                     preprocessor=None, stop_words=None, token_pattern=u\"[\\\\w']+\\\\w\\\\b\", \\\n",
      "                     analyzer=u'word', max_df=1.0, max_features=20000, vocabulary=None, \\\n",
      "                     binary=False, ngram_range=(1, 1), min_df=1, \\\n",
      "                     normalize=True):\n",
      "    \"\"\" Depuis un liste de documents, g\u00e9n\u00e8re une matrice sparse contenant les occurences des mots.\n",
      "        A chaque mot est associ\u00e9 un identifiant grace \u00e0 une table de hashage.\n",
      "    \"\"\"\n",
      "    vec_param = txt.CountVectorizer(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
      "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
      "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
      "                            min_df=min_df)\n",
      "    bow = fromVectoBow(all_docs, vec_param, normalize)\n",
      "    return bow, vec_param\n",
      "\n",
      "def fromVectoBow(all_docs, vec, normalize=True):\n",
      "    bow = vec.fit_transform(all_docs)\n",
      "    bow = bow.tocsr() # permet de print\n",
      "    if normalize:\n",
      "        bow = normalizeBow(bow)\n",
      "    return bow\n",
      "\n",
      "def normalizeBow(bow):\n",
      "    \"\"\" TFIDF : La somme de toutes les occurences des mots devient \u00e9gale \u00e0 1\n",
      "    \"\"\"\n",
      "    transformer = txt.TfidfTransformer(use_idf=False, smooth_idf=False)\n",
      "    bow_norm = transformer.fit_transform(bow)\n",
      "    return bow_norm   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Transformation inverse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse as sp\n",
      "\n",
      "def fromArgsToWords(args, vec):\n",
      "    \"\"\" A partir d'une liste d'arguments obtenus par l'extraction des coefficients de notre mod\u00e8le\n",
      "        (liste d'index de mots dans bow) et d'une fonction de vectorisation, rend une liste de mots.\n",
      "    \"\"\"\n",
      "    nb = len(args)\n",
      "    matrix = sp.coo_matrix((np.ones(nb), (np.zeros(nb),args)))\n",
      "    words = vec.inverse_transform(matrix)\n",
      "    return words\n",
      "\n",
      "def fromBowToWords(bow, vec):\n",
      "    \"\"\" A partir d'une matrice sparce, rend les mots associ\u00e9s aux identifiants g\u00e9n\u00e9r\u00e9s par\n",
      "        la fonction de hashage lors de la vectorisation.\n",
      "    \"\"\"\n",
      "    bow_inv = vec.inverse_transform(bow)\n",
      "    return bow_inv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Construction d'un classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import sklearn.naive_bayes as nb\n",
      "from sklearn import svm\n",
      "from sklearn import linear_model as lin\n",
      "from sklearn import cross_validation\n",
      "\n",
      "def crossValidation(clf, bow, all_labels, cv=5):\n",
      "    X = bow\n",
      "    y = all_labels\n",
      "    scores = cross_validation.cross_val_score(clf, X, y, cv=5)\n",
      "    np_scores = np.array(scores)\n",
      "    mean = np_scores.mean()\n",
      "    std = np_scores.std()\n",
      "    return scores, mean, std \n",
      "\n",
      "def fit(clf, bow, all_labels):\n",
      "    \"\"\" Indispensable pour obtenir les clf.coef_ utile \u00e0 la descrimination des mots \"\"\"\n",
      "    X = bow\n",
      "    y = all_labels\n",
      "    clf.fit(X, y)\n",
      "    return clf\n",
      "\n",
      "def predict(clf, docs):\n",
      "    return clf.predict(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mots les plus discriminants"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mostDescriminantWords(clf, vec, nb_words=100):\n",
      "    \"\"\" Test\u00e9 avec svm.LinearSVC() \"\"\"\n",
      "    args_sort = clf.coef_.reshape(clf.coef_.shape[1]).argsort() # index des mots tri\u00e9s par coef\n",
      "    args_pos = args_sort[:nb_words]\n",
      "    words_pos = fromArgsToWords(args_pos, vec)\n",
      "    args_neg = args_sort[-nb_words:]\n",
      "    words_neg = fromArgsToWords(args_neg, vec)\n",
      "    return words_pos, words_neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Main()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Chargement du corpus movies1000\n",
      "path = '/Users/Tamazy/Dropbox/_Docs/UPMC/TAL/TME3/movies1000'\n",
      "docs = getDocs(path)\n",
      "all_docs = getAllDocs(docs) # liste contenant l'ensemble des documents du corpus d'apprentissage\n",
      "all_labels = getAllLabels(docs)\n",
      "\n",
      "print \"Le contenu du premier document :\"\n",
      "print all_docs[0][:100]\n",
      "\n",
      "print \"Le label associ\u00e9 :\", all_labels[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Le contenu du premier document :\n",
        "plot : two teen couples go to a church party , drink and then drive . \n",
        "they get into an accident . \n",
        "\n",
        "Le label associ\u00e9 : 0.0\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Param\u00e9trage\n",
      "languages = ['french', 'english', 'german', 'spanish']\n",
      "stop_words = makeNltkStopWords(languages) # si erreur executez nltkDownload()\n",
      "analyzer = u'word' # {\u2018word\u2019, \u2018char\u2019, \u2018char_wb\u2019}\n",
      "ngram_range = (1, 1) # unigrammes\n",
      "lowercase = True\n",
      "token_pattern = u\"[\\\\w']+\\\\w\\\\b\" # \n",
      "max_df = 1.0 #default\n",
      "min_df = 5. * 1./len(all_docs) # on enleve les mots qui apparaissent moins de 5 fois\n",
      "max_features = 20000 # nombre de mots au total dans notre matrice sparse\n",
      "binary = False # presence coding or counting\n",
      "strip_accents = u'ascii' #  {\u2018ascii\u2019, \u2018unicode\u2019, None}\n",
      "preprocessor=None\n",
      "vocabulary=None\n",
      "\n",
      "# Vectorisation\n",
      "bow, vec = fromAllDocsToBow(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
      "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
      "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
      "                            min_df=min_df)\n",
      "\n",
      "print \"Mots vectoris\u00e9s du second document :\"\n",
      "print bow[1]\n",
      "\n",
      "# /!\\ Le premier indice est toujours 0 si on print bow[i]\n",
      "# /!\\ alors que si on print bow le premier indice changera\n",
      "# /!\\ en fonction du document"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mots vectoris\u00e9s du second document :\n",
        "  (0, 5084)\t0.0678844233302\n",
        "  (0, 7857)\t0.339422116651\n",
        "  (0, 9993)\t0.0678844233302\n",
        "  (0, 4158)\t0.0678844233302\n",
        "  (0, 5548)\t0.0678844233302\n",
        "  (0, 5206)\t0.0678844233302\n",
        "  (0, 9159)\t0.0678844233302\n",
        "  (0, 13102)\t0.0678844233302\n",
        "  (0, 6396)\t0.203653269991\n",
        "  (0, 5188)\t0.13576884666\n",
        "  (0, 989)\t0.0678844233302\n",
        "  (0, 7001)\t0.203653269991\n",
        "  (0, 1999)\t0.0678844233302\n",
        "  (0, 3583)\t0.203653269991\n",
        "  (0, 5228)\t0.0678844233302\n",
        "  (0, 1245)\t0.0678844233302\n",
        "  (0, 9601)\t0.13576884666\n",
        "  (0, 11495)\t0.0678844233302\n",
        "  (0, 11365)\t0.0678844233302\n",
        "  (0, 7066)\t0.0678844233302\n",
        "  (0, 11435)\t0.0678844233302\n",
        "  (0, 12126)\t0.0678844233302\n",
        "  (0, 8693)\t0.0678844233302\n",
        "  (0, 6725)\t0.271537693321\n",
        "  (0, 12222)\t0.0678844233302\n",
        "  :\t:\n",
        "  (0, 7007)\t0.0678844233302\n",
        "  (0, 13395)\t0.0678844233302\n",
        "  (0, 7006)\t0.0678844233302\n",
        "  (0, 13297)\t0.0678844233302\n",
        "  (0, 5437)\t0.0678844233302\n",
        "  (0, 5407)\t0.0678844233302\n",
        "  (0, 13025)\t0.0678844233302\n",
        "  (0, 5547)\t0.0678844233302\n",
        "  (0, 9588)\t0.0678844233302\n",
        "  (0, 11352)\t0.0678844233302\n",
        "  (0, 11337)\t0.0678844233302\n",
        "  (0, 10122)\t0.0678844233302\n",
        "  (0, 3256)\t0.0678844233302\n",
        "  (0, 1933)\t0.0678844233302\n",
        "  (0, 8257)\t0.0678844233302\n",
        "  (0, 10762)\t0.0678844233302\n",
        "  (0, 8810)\t0.0678844233302\n",
        "  (0, 11105)\t0.0678844233302\n",
        "  (0, 1471)\t0.0678844233302\n",
        "  (0, 1373)\t0.0678844233302\n",
        "  (0, 8597)\t0.0678844233302\n",
        "  (0, 12460)\t0.0678844233302\n",
        "  (0, 5633)\t0.0678844233302\n",
        "  (0, 8416)\t0.0678844233302\n",
        "  (0, 7870)\t0.0678844233302\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.py:123: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
        "  tokens = [w for w in tokens if w not in stop_words]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalisation\n",
      "bow = normalizeBow(bow)\n",
      "\n",
      "print \"Apr\u00e8s normalisation :\"\n",
      "print bow[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Apr\u00e8s normalisation :\n",
        "  (0, 5084)\t0.0678844233302\n",
        "  (0, 7857)\t0.339422116651\n",
        "  (0, 9993)\t0.0678844233302\n",
        "  (0, 4158)\t0.0678844233302\n",
        "  (0, 5548)\t0.0678844233302\n",
        "  (0, 5206)\t0.0678844233302\n",
        "  (0, 9159)\t0.0678844233302\n",
        "  (0, 13102)\t0.0678844233302\n",
        "  (0, 6396)\t0.203653269991\n",
        "  (0, 5188)\t0.13576884666\n",
        "  (0, 989)\t0.0678844233302\n",
        "  (0, 7001)\t0.203653269991\n",
        "  (0, 1999)\t0.0678844233302\n",
        "  (0, 3583)\t0.203653269991\n",
        "  (0, 5228)\t0.0678844233302\n",
        "  (0, 1245)\t0.0678844233302\n",
        "  (0, 9601)\t0.13576884666\n",
        "  (0, 11495)\t0.0678844233302\n",
        "  (0, 11365)\t0.0678844233302\n",
        "  (0, 7066)\t0.0678844233302\n",
        "  (0, 11435)\t0.0678844233302\n",
        "  (0, 12126)\t0.0678844233302\n",
        "  (0, 8693)\t0.0678844233302\n",
        "  (0, 6725)\t0.271537693321\n",
        "  (0, 12222)\t0.0678844233302\n",
        "  :\t:\n",
        "  (0, 7007)\t0.0678844233302\n",
        "  (0, 13395)\t0.0678844233302\n",
        "  (0, 7006)\t0.0678844233302\n",
        "  (0, 13297)\t0.0678844233302\n",
        "  (0, 5437)\t0.0678844233302\n",
        "  (0, 5407)\t0.0678844233302\n",
        "  (0, 13025)\t0.0678844233302\n",
        "  (0, 5547)\t0.0678844233302\n",
        "  (0, 9588)\t0.0678844233302\n",
        "  (0, 11352)\t0.0678844233302\n",
        "  (0, 11337)\t0.0678844233302\n",
        "  (0, 10122)\t0.0678844233302\n",
        "  (0, 3256)\t0.0678844233302\n",
        "  (0, 1933)\t0.0678844233302\n",
        "  (0, 8257)\t0.0678844233302\n",
        "  (0, 10762)\t0.0678844233302\n",
        "  (0, 8810)\t0.0678844233302\n",
        "  (0, 11105)\t0.0678844233302\n",
        "  (0, 1471)\t0.0678844233302\n",
        "  (0, 1373)\t0.0678844233302\n",
        "  (0, 8597)\t0.0678844233302\n",
        "  (0, 12460)\t0.0678844233302\n",
        "  (0, 5633)\t0.0678844233302\n",
        "  (0, 8416)\t0.0678844233302\n",
        "  (0, 7870)\t0.0678844233302\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mod\u00e8les\n",
      "clf = svm.LinearSVC() # SVM\n",
      "clf_nb = nb.MultinomialNB() # Naive Bayes\n",
      "clf_rl = lin.LogisticRegression() # regression logistique\n",
      "\n",
      "# Cross-Validation\n",
      "scores, mean, std  = crossValidation(clf, bow, all_labels, cv=5)\n",
      "\n",
      "print \"Scores obtenus avec crossValidation :\", scores\n",
      "print \"Moyenne :\", mean\n",
      "print \"Ecart type :\", std"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Scores obtenus avec crossValidation : [ 0.8425  0.85    0.8375  0.8525  0.87  ]\n",
        "Moyenne : 0.8505\n",
        "Ecart type : 0.0111130553854\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mots les plus discriminants\n",
      "clf = fit(clf, bow, all_labels) # afin de pouvoir r\u00e9cup\u00e9rer les coefficients du clf\n",
      "words_pos, words_neg = mostDescriminantWords(clf, vec, nb_words=100)\n",
      "print \"Mots les plus discriminants\"\n",
      "print \"bad] Pour d\u00e9crire les mauvais films: \", words_pos\n",
      "print \"good] Pour d\u00e9crire les bons films: \", words_neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mots les plus discriminants\n",
        "bad] Pour d\u00e9crire les mauvais films:  [array([u'8mm', u'actors', u'adam', u'annoying', u'anyway', u'apparently',\n",
        "       u'attempt', u'attempts', u'audience', u'awful', u'bad', u'better',\n",
        "       u'bland', u'boring', u'cage', u'catch', u'cheap', u'complete',\n",
        "       u'could', u'data', u'designed', u\"didn't\", u'director',\n",
        "       u'disappointing', u'dull', u'dutch', u'eddie', u'else', u'enough',\n",
        "       u'even', u'fails', u'falls', u'female', u'figure', u'filmmakers',\n",
        "       u'flat', u'given', u'godzilla', u'guess', u'happens', u'harry',\n",
        "       u'headed', u'idea', u'jesse', u'joke', u'lame', u'laughable',\n",
        "       u'least', u'looks', u'made', u'make', u'mars', u'material',\n",
        "       u'maybe', u'meant', u'mess', u'metro', u'minute', u'naked', u'none',\n",
        "       u'nothing', u'obvious', u'pay', u'plot', u'point', u'pointless',\n",
        "       u'poor', u'poorly', u'potential', u'reason', u'ridiculous', u'save',\n",
        "       u'saved', u'script', u'seemed', u'sports', u'stupid', u'subplot',\n",
        "       u'superior', u'supposed', u'talks', u'tedious', u'terrible',\n",
        "       u\"there's\", u'tries', u'tv', u'unfortunately', u'video', u'waste',\n",
        "       u'wasted', u'weak', u'west', u'woman', u'women', u'wonder', u'work',\n",
        "       u'worse', u'worst', u'would', u'write'], \n",
        "      dtype='<U17')]\n",
        "good] Pour d\u00e9crire les bons films:  [array([u'allows', u'although', u'always', u'america', u'american', u'back',\n",
        "       u'ben', u'best', u'bit', u'bond', u'brought', u'cameron', u'change',\n",
        "       u'chemistry', u'class', u'comic', u'contrast', u'damon', u'dark',\n",
        "       u'definitely', u'different', u'easily', u'ed', u'edge', u'ending',\n",
        "       u'enjoy', u'enjoyable', u'entertaining', u'especially',\n",
        "       u'everything', u'excellent', u'extremely', u'fantastic', u'first',\n",
        "       u'flaws', u'follows', u'frank', u'fun', u'gas', u'gives', u'good',\n",
        "       u'great', u'hilarious', u'horror', u'husband', u\"i've\", u'italian',\n",
        "       u'jackie', u'job', u'life', u'makes', u'many', u'masterpiece',\n",
        "       u'matrix', u'memorable', u'mike', u'movies', u'mulan', u'others',\n",
        "       u'outstanding', u'overall', u'pace', u'people', u'perfect',\n",
        "       u'perfectly', u'performance', u'performances', u'plenty',\n",
        "       u'political', u'pulp', u'quite', u'realistic', u'relationship',\n",
        "       u'religion', u'rocky', u'see', u'seen', u\"smith's\", u'social',\n",
        "       u'solid', u'sometimes', u'song', u'strange', u'takes', u'terrific',\n",
        "       u'thankfully', u'titanic', u'together', u'trek', u'true', u'truman',\n",
        "       u'truth', u'using', u'view', u'violence', u'well', u'without',\n",
        "       u'wonderfully', u'works', u'yet'], \n",
        "      dtype='<U17')]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comment = \"\"\"Automata' (2014) is a critically underrated and atmospheric science- fiction thriller in the same vein as 'I Robot' and 'Blade Runner'. It boasts excellent visual effects, as well as an engaging and intelligent story. While it borrows from other science fiction it does so successfully, especially the atmospheric and decaying world we're thrusts into from the beginning.\n",
      "The story centers around Antonio Banderas's character, Jacq Vaucan - a world-weary insurance agent for a robotics corporation whose job is to investigate robots violating their protocols which are one: harming any form of life, and two: they can neither repair themselves nor alter another robot in any fashion. On the trail of a robot Vaucan discovers a robot stealing parts in an apparent attempt to alter itself. This leads him to the clock master - a fixer who may have just succeeded the second protocol.\n",
      "Automata is a throwback to thoughtful science fiction. It's not for the feint of heart but if you're engaged and buy into the world and the premise then you'll be rewarded. The film surprised me in a lot of ways\n",
      "especially for such a relatively small budget but imagery is fantastic and the effects are mostly practical, and built with little computer generated imagery save for some backgrounds and action scenes which make it that much more realistic.\n",
      "It's slower and probably has less action but if we're comparing it to what it will inevitably be compared to, 'I Robot', Automata is a better movie. More thoughtful, grittier and executed a whole lot better visually. It's not a perfect flick by any means but it's worth watching and deciding for yourself.\"\"\"\n",
      "\n",
      "com_bow = fromVectoBow([comment], vec, normalize=True)\n",
      "print com_bow\n",
      "print fromBowToWords(com_bow, vec)\n",
      "\n",
      "pred = predict(clf, com_bow)\n",
      "print \"Classe du commentaire :\", pred\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 12)\t0.19695964929\n",
        "  (0, 2)\t0.0656532164299\n",
        "  (0, 29)\t0.0656532164299\n",
        "  (0, 105)\t0.0656532164299\n",
        "  (0, 10)\t0.13130643286\n",
        "  (0, 90)\t0.19695964929\n",
        "  (0, 42)\t0.19695964929\n",
        "  (0, 100)\t0.0656532164299\n",
        "  (0, 107)\t0.0656532164299\n",
        "  (0, 1)\t0.13130643286\n",
        "  (0, 84)\t0.328266082149\n",
        "  (0, 0)\t0.0656532164299\n",
        "  (0, 87)\t0.0656532164299\n",
        "  (0, 17)\t0.0656532164299\n",
        "  (0, 37)\t0.0656532164299\n",
        "  (0, 109)\t0.0656532164299\n",
        "  (0, 33)\t0.13130643286\n",
        "  (0, 115)\t0.0656532164299\n",
        "  (0, 35)\t0.0656532164299\n",
        "  (0, 54)\t0.0656532164299\n",
        "  (0, 95)\t0.13130643286\n",
        "  (0, 18)\t0.0656532164299\n",
        "  (0, 97)\t0.0656532164299\n",
        "  (0, 36)\t0.13130643286\n",
        "  (0, 30)\t0.0656532164299\n",
        "  :\t:\n",
        "  (0, 88)\t0.0656532164299\n",
        "  (0, 13)\t0.0656532164299\n",
        "  (0, 3)\t0.13130643286\n",
        "  (0, 89)\t0.0656532164299\n",
        "  (0, 64)\t0.0656532164299\n",
        "  (0, 70)\t0.0656532164299\n",
        "  (0, 80)\t0.0656532164299\n",
        "  (0, 92)\t0.0656532164299\n",
        "  (0, 77)\t0.0656532164299\n",
        "  (0, 60)\t0.0656532164299\n",
        "  (0, 26)\t0.0656532164299\n",
        "  (0, 52)\t0.0656532164299\n",
        "  (0, 25)\t0.0656532164299\n",
        "  (0, 16)\t0.13130643286\n",
        "  (0, 69)\t0.0656532164299\n",
        "  (0, 48)\t0.0656532164299\n",
        "  (0, 38)\t0.0656532164299\n",
        "  (0, 116)\t0.0656532164299\n",
        "  (0, 110)\t0.0656532164299\n",
        "  (0, 74)\t0.0656532164299\n",
        "  (0, 45)\t0.0656532164299\n",
        "  (0, 67)\t0.0656532164299\n",
        "  (0, 119)\t0.0656532164299\n",
        "  (0, 111)\t0.0656532164299\n",
        "  (0, 31)\t0.0656532164299\n",
        "[array([u'automata', u'2014', u'critically', u'underrated', u'atmospheric',\n",
        "       u'science', u'fiction', u'thriller', u'vein', u\"'i\", u'robot',\n",
        "       u\"'blade\", u'runner', u'boasts', u'excellent', u'visual',\n",
        "       u'effects', u'well', u'engaging', u'intelligent', u'story',\n",
        "       u'borrows', u'successfully', u'especially', u'decaying', u'world',\n",
        "       u\"we're\", u'thrusts', u'beginning', u'centers', u'around',\n",
        "       u'antonio', u\"banderas's\", u'character', u'jacq', u'vaucan',\n",
        "       u'weary', u'insurance', u'agent', u'robotics', u'corporation',\n",
        "       u'whose', u'job', u'investigate', u'robots', u'violating',\n",
        "       u'protocols', u'one', u'harming', u'form', u'life', u'two',\n",
        "       u'neither', u'repair', u'alter', u'another', u'fashion', u'trail',\n",
        "       u'discovers', u'stealing', u'parts', u'apparent', u'attempt',\n",
        "       u'leads', u'clock', u'master', u'fixer', u'may', u'succeeded',\n",
        "       u'second', u'protocol', u'throwback', u'thoughtful', u\"it's\",\n",
        "       u'feint', u'heart', u\"you're\", u'engaged', u'buy', u'premise',\n",
        "       u\"you'll\", u'rewarded', u'film', u'surprised', u'lot', u'ways',\n",
        "       u'relatively', u'small', u'budget', u'imagery', u'fantastic',\n",
        "       u'mostly', u'practical', u'built', u'little', u'computer',\n",
        "       u'generated', u'save', u'backgrounds', u'action', u'scenes',\n",
        "       u'make', u'much', u'realistic', u'slower', u'probably', u'less',\n",
        "       u'comparing', u'inevitably', u'compared', u'better', u'movie',\n",
        "       u'grittier', u'executed', u'whole', u'visually', u'perfect',\n",
        "       u'flick', u'means', u'worth', u'watching', u'deciding'], \n",
        "      dtype='<U12')]\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "X has 122 features per sample; expecting 13432",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-13-d53c204d5e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfromBowToWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcom_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Classe du commentaire :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-6-7c55065de5c9>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(clf, docs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \"\"\"\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 196\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
        "\u001b[0;31mValueError\u001b[0m: X has 122 features per sample; expecting 13432"
       ]
      }
     ],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}