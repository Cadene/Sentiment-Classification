{
 "metadata": {
  "name": "",
  "signature": "sha256:3764d73570fc38f87f5c9effbe9a1d3545ba2ecfa576c4c91a6da6cc4d6189e1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TAL - TME3 - Classification de documents"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Chargement du corpus movies1000 (lecture de fichiers)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import nltk.corpus.reader as pt\n",
      "\n",
      "def getDocs(path):\n",
      "    rdr = pt.CategorizedPlaintextCorpusReader(path, r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n",
      "    docs = [[rdr.raw(fileids=[f]) for f in rdr.fileids(c) ] for c in rdr.categories()]\n",
      "    return docs\n",
      "\n",
      "def getAllDocs(docs):\n",
      "    all_docs = docs[0] + docs[1]\n",
      "    return all_docs\n",
      "\n",
      "def getAllLabels(docs):\n",
      "    len_sad = len(docs[0])\n",
      "    len_happy = len(docs[1])\n",
      "    all_labels = np.ones(len_sad + len_happy)\n",
      "    all_labels[:len_happy] = 0\n",
      "    return all_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cr\u00e9ation des stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import download\n",
      "\n",
      "def nltkDownload():\n",
      "    \"\"\" Vous n'aurez surement pas les stopwords, il vous faudra les t\u00e9l\u00e9charger.\n",
      "    \"\"\"\n",
      "    download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "\n",
      "def makeNltkStopWords(languages=['french', 'english', 'german', 'spanish']):\n",
      "    stop_words = []\n",
      "    for l in languages:\n",
      "        for w in stopwords.words(l):\n",
      "           stop_words.append(w.encode('utf-8')) #w.decode('utf-8') buggait... avec certains caract\u00e8res\n",
      "    return stop_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Vectorisation et normalisation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.feature_extraction.text as txt\n",
      "    \n",
      "def fromAllDocsToBow(all_docs, strip_accents=u'ascii', lowercase=True, \\\n",
      "                     preprocessor=None, stop_words=None, token_pattern=u\"[\\\\w']+\\\\w\\\\b\", \\\n",
      "                     analyzer=u'word', max_df=1.0, max_features=20000, vocabulary=None, \\\n",
      "                     binary=True, ngram_range=(1, 1), min_df=0.0025):\n",
      "    \"\"\" Depuis un liste de documents, g\u00e9n\u00e8re une matrice sparse contenant les occurences des mots.\n",
      "        A chaque mot est associ\u00e9 un identifiant grace \u00e0 une table de hashage.\n",
      "    \"\"\"\n",
      "    vec_param = txt.CountVectorizer(all_docs)\n",
      "#strip_accents, lowercase, preprocessor, stop_words, token_pattern, vocabulary, binary, ngram_range)#, min_df)\n",
      "    # analyzer, max_df, max_features\n",
      "    bow = vec_param.fit_transform(all_docs)\n",
      "    bow = bow.tocsr() # pour pouvoir print\n",
      "    return bow, vec_param\n",
      "\n",
      "def normalizeBow(bow):\n",
      "    \"\"\" La somme de toutes les occurences des mots devient \u00e9gale \u00e0 1\n",
      "    \"\"\"\n",
      "    transformer = txt.TfidfTransformer(use_idf=False, smooth_idf=False)\n",
      "    bow_norm = transformer.fit_transform(bow)\n",
      "    return bow_norm   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Transformation inverse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse as sp\n",
      "\n",
      "def fromArgsToWords(args, vec):\n",
      "    \"\"\" A partir d'une liste d'arguments obtenus par l'extraction des coefficients de notre mod\u00e8le\n",
      "        et d'une fonction de vectorisation, rend une liste de mots.\n",
      "    \"\"\"\n",
      "    nb = len(args)\n",
      "    matrix = sp.coo_matrix((np.ones(nb), (np.zeros(nb),args)))\n",
      "    words = vec.inverse_transform(matrix)\n",
      "    return words\n",
      "\n",
      "def fromBowToWords(bow, vec):\n",
      "    \"\"\" A partir d'une matrice sparce, rend les mots associ\u00e9s aux identifiants g\u00e9n\u00e9r\u00e9s par\n",
      "        la fonction de hashage lors de la vectorisation.\n",
      "    \"\"\"\n",
      "    bow_inv = vec.inverse_transform(bow)\n",
      "    return bow_inv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Construction d'un classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import sklearn.naive_bayes as nb\n",
      "from sklearn import svm\n",
      "from sklearn import linear_model as lin\n",
      "from sklearn import cross_validation\n",
      "\n",
      "def crossValidation(clf, bow, all_labels, cv=5):\n",
      "    X = bow\n",
      "    y = all_labels\n",
      "    scores = cross_validation.cross_val_score(clf, X, y, cv=5)\n",
      "    np_scores = np.array(scores)\n",
      "    mean = np_scores.mean()\n",
      "    std = np_scores.std()\n",
      "    return scores, mean, std \n",
      "\n",
      "def fit(clf, bow, all_labels):\n",
      "    \"\"\" Indispensable pour obtenir les clf.coef_ utile \u00e0 la descrimination des mots \"\"\"\n",
      "    X = bow\n",
      "    y = all_labels\n",
      "    clf.fit(X, y)\n",
      "    return clf\n",
      "\n",
      "def predict(clf, docs):\n",
      "    return clf.predict(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mots les plus discriminants"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mostDescriminantWords(clf, vec, nb_words=100):\n",
      "    \"\"\" Test\u00e9 avec svm.LinearSVC() \"\"\"\n",
      "    args_sort = clf.coef_.reshape(clf.coef_.shape[1]).argsort()\n",
      "    args_pos = args_sort[:nb_words]\n",
      "    words_pos = fromArgsToWords(args_pos, vec)\n",
      "    args_neg = args_sort[-nb_words:]\n",
      "    words_neg = fromArgsToWords(args_neg, vec)\n",
      "    return words_pos, words_neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Main()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Chargement du corpus movies1000\n",
      "path = '/Users/Tamazy/Dropbox/_Docs/UPMC/TAL/TME3/movies1000'\n",
      "docs = getDocs(path)\n",
      "all_docs = getAllDocs(docs) # liste contenant l'ensemble des documents du corpus d'apprentissage\n",
      "all_labels = getAllLabels(docs)\n",
      "\n",
      "print \"Le contenu du premier document :\"\n",
      "print all_docs[0][:100]\n",
      "\n",
      "print \"Le label associ\u00e9 :\", all_labels[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Le contenu du premier document :\n",
        "plot : two teen couples go to a church party , drink and then drive . \n",
        "they get into an accident . \n",
        "\n",
        "Le label associ\u00e9 : 0.0\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Param\u00e9trage\n",
      "languages = ['french', 'english', 'german', 'spanish']\n",
      "stop_words = makeNltkStopWords(languages) # si erreur executez nltkDownload()\n",
      "analyzer = u'word' # {\u2018word\u2019, \u2018char\u2019, \u2018char_wb\u2019}\n",
      "ngram_range = (1, 1) # unigrammes\n",
      "lowercase = True\n",
      "token_pattern = u\"[\\\\w']+\\\\w\\\\b\" # \n",
      "max_df = 1.0 #default\n",
      "min_df = 5. * 1./len(all_docs) # on enleve les mots qui apparaissent moins de 5 fois\n",
      "max_features = 20000 # nombre de mots au total dans notre matrice sparse\n",
      "binary = True # presence coding or counting\n",
      "strip_accents = u'ascii' #  {\u2018ascii\u2019, \u2018unicode\u2019, None}\n",
      "preprocessor=None\n",
      "vocabulary=None\n",
      "\n",
      "# Vectorisation\n",
      "bow, vec = fromAllDocsToBow(all_docs, strip_accents, lowercase, preprocessor, \\\n",
      "                            stop_words, token_pattern, analyzer, max_df, max_features, \\\n",
      "                            vocabulary, binary, ngram_range, min_df)\n",
      "\n",
      "print \"Mots vectoris\u00e9s du second document :\"\n",
      "print bow[1]\n",
      "\n",
      "# /!\\ Le premier indice est toujours 0 si on print bow[i]\n",
      "# /!\\ alors que si on print bow le premier indice changera\n",
      "# /!\\ en fonction du document"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mots vectoris\u00e9s du second document :\n",
        "  (0, 35714)\t2\n",
        "  (0, 1810)\t8\n",
        "  (0, 35351)\t2\n",
        "  (0, 14630)\t1\n",
        "  (0, 18386)\t1\n",
        "  (0, 24386)\t8\n",
        "  (0, 35280)\t13\n",
        "  (0, 17608)\t4\n",
        "  (0, 16315)\t1\n",
        "  (0, 15965)\t1\n",
        "  (0, 38678)\t1\n",
        "  (0, 23113)\t5\n",
        "  (0, 24793)\t2\n",
        "  (0, 13695)\t2\n",
        "  (0, 35277)\t4\n",
        "  (0, 24501)\t3\n",
        "  (0, 37843)\t1\n",
        "  (0, 18630)\t4\n",
        "  (0, 18583)\t3\n",
        "  (0, 35396)\t2\n",
        "  (0, 29438)\t1\n",
        "  (0, 12134)\t1\n",
        "  (0, 39013)\t1\n",
        "  (0, 39482)\t1\n",
        "  (0, 16076)\t1\n",
        "  :\t:\n",
        "  (0, 28303)\t1\n",
        "  (0, 20499)\t1\n",
        "  (0, 39143)\t1\n",
        "  (0, 15719)\t1\n",
        "  (0, 15633)\t1\n",
        "  (0, 38418)\t1\n",
        "  (0, 28336)\t1\n",
        "  (0, 33388)\t1\n",
        "  (0, 33358)\t1\n",
        "  (0, 38965)\t1\n",
        "  (0, 29786)\t1\n",
        "  (0, 9454)\t1\n",
        "  (0, 30673)\t1\n",
        "  (0, 5931)\t1\n",
        "  (0, 24321)\t1\n",
        "  (0, 31663)\t1\n",
        "  (0, 26133)\t1\n",
        "  (0, 4572)\t1\n",
        "  (0, 17290)\t1\n",
        "  (0, 4251)\t1\n",
        "  (0, 25441)\t1\n",
        "  (0, 36492)\t1\n",
        "  (0, 24773)\t1\n",
        "  (0, 23161)\t1\n",
        "  (0, 34250)\t1\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalisation\n",
      "bow = normalizeBow(bow)\n",
      "\n",
      "print \"Apr\u00e8s normalisation :\"\n",
      "print bow[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Apr\u00e8s normalisation :\n",
        "  (0, 35714)\t0.0778498944162\n",
        "  (0, 1810)\t0.311399577665\n",
        "  (0, 35351)\t0.0778498944162\n",
        "  (0, 14630)\t0.0389249472081\n",
        "  (0, 18386)\t0.0389249472081\n",
        "  (0, 24386)\t0.311399577665\n",
        "  (0, 35280)\t0.506024313705\n",
        "  (0, 17608)\t0.155699788832\n",
        "  (0, 16315)\t0.0389249472081\n",
        "  (0, 15965)\t0.0389249472081\n",
        "  (0, 38678)\t0.0389249472081\n",
        "  (0, 23113)\t0.19462473604\n",
        "  (0, 24793)\t0.0778498944162\n",
        "  (0, 13695)\t0.0778498944162\n",
        "  (0, 35277)\t0.155699788832\n",
        "  (0, 24501)\t0.116774841624\n",
        "  (0, 37843)\t0.0389249472081\n",
        "  (0, 18630)\t0.155699788832\n",
        "  (0, 18583)\t0.116774841624\n",
        "  (0, 35396)\t0.0778498944162\n",
        "  (0, 29438)\t0.0389249472081\n",
        "  (0, 12134)\t0.0389249472081\n",
        "  (0, 39013)\t0.0389249472081\n",
        "  (0, 39482)\t0.0389249472081\n",
        "  (0, 16076)\t0.0389249472081\n",
        "  :\t:\n",
        "  (0, 28303)\t0.0389249472081\n",
        "  (0, 20499)\t0.0389249472081\n",
        "  (0, 39143)\t0.0389249472081\n",
        "  (0, 15719)\t0.0389249472081\n",
        "  (0, 15633)\t0.0389249472081\n",
        "  (0, 38418)\t0.0389249472081\n",
        "  (0, 28336)\t0.0389249472081\n",
        "  (0, 33388)\t0.0389249472081\n",
        "  (0, 33358)\t0.0389249472081\n",
        "  (0, 38965)\t0.0389249472081\n",
        "  (0, 29786)\t0.0389249472081\n",
        "  (0, 9454)\t0.0389249472081\n",
        "  (0, 30673)\t0.0389249472081\n",
        "  (0, 5931)\t0.0389249472081\n",
        "  (0, 24321)\t0.0389249472081\n",
        "  (0, 31663)\t0.0389249472081\n",
        "  (0, 26133)\t0.0389249472081\n",
        "  (0, 4572)\t0.0389249472081\n",
        "  (0, 17290)\t0.0389249472081\n",
        "  (0, 4251)\t0.0389249472081\n",
        "  (0, 25441)\t0.0389249472081\n",
        "  (0, 36492)\t0.0389249472081\n",
        "  (0, 24773)\t0.0389249472081\n",
        "  (0, 23161)\t0.0389249472081\n",
        "  (0, 34250)\t0.0389249472081\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mod\u00e8les\n",
      "clf = svm.LinearSVC() # SVM\n",
      "clf_nb = nb.MultinomialNB() # Naive Bayes\n",
      "clf_rl = lin.LogisticRegression() # regression logistique\n",
      "\n",
      "# Cross-Validation\n",
      "scores, mean, std  = crossValidation(clf, bow, all_labels, cv=5)\n",
      "\n",
      "print \"Scores obtenus avec crossValidation :\", scores\n",
      "print \"Moyenne :\", mean\n",
      "print \"Ecart type :\", std"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Scores obtenus avec crossValidation : [ 0.8025  0.84    0.83    0.85    0.84  ]\n",
        "Moyenne : 0.8325\n",
        "Ecart type : 0.0162788205961\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mots les plus discriminants\n",
      "clf = fit(clf, bow, all_labels) # afin de pouvoir r\u00e9cup\u00e9rer les coefficients du clf\n",
      "words_pos, words_neg = mostDescriminantWords(clf, vec, nb_words=100)\n",
      "print \"Mots les plus discriminants\"\n",
      "print \"bad] Pour d\u00e9crire les mauvais films: \", words_pos\n",
      "print \"good] Pour d\u00e9crire les bons films: \", words_neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mots les plus discriminants\n",
        "bad] Pour d\u00e9crire les mauvais films:  [array([u'acting', u'adam', u'andrew', u'annoying', u'any', u'anyway',\n",
        "       u'attempt', u'attempts', u'awful', u'bad', u'becomes', u'boring',\n",
        "       u'brother', u'carpenter', u'century', u'cheap', u'could',\n",
        "       u'designed', u'disappointing', u'do', u'dutch', u'else', u'enough',\n",
        "       u'eve', u'even', u'extraordinarily', u'falls', u'female',\n",
        "       u'filmmakers', u'flat', u'giant', u'given', u'greed', u'guess',\n",
        "       u'harry', u'headed', u'here', u'heston', u'horrendous', u'humor',\n",
        "       u'hurlyburly', u'jakob', u'jesse', u'joke', u'julie', u'lame',\n",
        "       u'long', u'looks', u'marc', u'mario', u'maybe', u'mess', u'metro',\n",
        "       u'minute', u'more', u'naked', u'nothing', u'only', u'part', u'plot',\n",
        "       u'point', u'pointless', u'poor', u'poorly', u'potential',\n",
        "       u'questions', u'reason', u'reeves', u'result', u'ridiculous',\n",
        "       u'sandler', u'saved', u'scary', u'script', u'seemed', u'self',\n",
        "       u'series', u'should', u'since', u'somewhere', u'stereotypical',\n",
        "       u'stupid', u'style', u'superior', u'supposed', u'talent',\n",
        "       u'tedious', u'terrible', u'thomas', u'tv', u'unfortunately',\n",
        "       u'waste', u'why', u'williams', u'woman', u'women', u'words',\n",
        "       u'work', u'worst', u'write'], \n",
        "      dtype='<U58')]\n",
        "good] Pour d\u00e9crire les bons films:  [array([u'above', u'allows', u'also', u'american', u'argento', u'ash',\n",
        "       u'back', u'beavis', u'ben', u'best', u'bit', u'blade', u'both',\n",
        "       u'burton', u'cameron', u'change', u'color', u'common', u'dark',\n",
        "       u'days', u'definitely', u'different', u'due', u'enjoy',\n",
        "       u'enjoyable', u'enjoyed', u'entertaining', u'equally', u'era',\n",
        "       u'especially', u'everything', u'excellent', u'filmmaking', u'flaws',\n",
        "       u'follows', u'force', u'fraser', u'fresh', u'fun', u'gas', u'gay',\n",
        "       u'great', u'happy', u'help', u'hilarious', u'hopkins', u'husband',\n",
        "       u'important', u'job', u'karen', u'knows', u'laughs', u'loved',\n",
        "       u'luckily', u'makes', u'marcy', u'masterpiece', u'matrix', u'mel',\n",
        "       u'memorable', u'memory', u'mulan', u'others', u'overall', u'pace',\n",
        "       u'particularly', u'people', u'perfect', u'perfectly',\n",
        "       u'performances', u'pulp', u'queen', u'quite', u'rocky', u'see',\n",
        "       u'seen', u'several', u'sherri', u'similar', u'solid', u'sometimes',\n",
        "       u'son', u'strange', u'sweet', u'takes', u'terrific', u'thankfully',\n",
        "       u'top', u'true', u'twister', u'using', u'very', u'view',\n",
        "       u'violence', u'wahlberg', u'well', u'while', u'without', u'works',\n",
        "       u'yet'], \n",
        "      dtype='<U58')]\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}